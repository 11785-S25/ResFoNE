2025-03-11 17:36:09 - INFO - result/pretrained/gpt2/language_math_10base/fne/period_base_list_[10.0]_batchsize_32_epochs_5_lr_5e-05_int_digit_len_5_frac_digit_len_5_seed42_num_train_samples_None_model_size_level_-1_lengensize_0_addlinear_True
2025-03-11 17:36:09 - INFO - Namespace(batch_size=32, epochs=5, int_digit_len=5, frac_digit_len=5, len_gen_size=0, lr=5e-05, name='', model='gpt2', dataset='Onlydrinkwater/language_math_10base', train_from_scratch=False, use_digit_wise_tokenizer=False, num_train_samples=None, num_test_samples=None, seed=42, model_size_level=-1, method='fne', scheduler_name='cosine', period_base_list=[10.0], clip=False, not_add_linear=False, add_linear=True)
2025-03-11 17:36:09 - INFO - Dataset specified as single name: Onlydrinkwater/language_math_10base
2025-03-11 17:36:46 - INFO - Pre-trained model loaded!
2025-03-11 17:36:48 - INFO - Standard tokenizer loaded.
2025-03-11 17:36:48 - INFO - Expanding model config from 50257 to 50259 to match tokenizer.
2025-03-11 17:36:49 - INFO - Actual model size (total parameters): 124.44M
2025-03-11 17:36:49 - INFO - Padding token ID: 50257
2025-03-11 17:36:49 - INFO - [NUM] token ID: 50258
2025-03-11 17:36:51 - INFO - Train dataset length: 27352, Test dataset length: 3420
2025-03-11 17:36:53 - INFO - 2 data example: [{'input_ids': tensor([14957,   286,   220,   220, 50258,   220,   290,   220,   220, 50258,
          220]), 'numbers': [177.0, 219.0], 'label': 396.0}, {'input_ids': tensor([ 9771,  3129,   378,   220,   220, 50258,   220,  1343,   220,   220,
        50258,   220]), 'numbers': [71.0, 93.0], 'label': 164.0}]
2025-03-11 17:36:53 - INFO - period_list: [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0, 100000.0]
2025-03-11 17:36:53 - INFO - self.powers_of_ten: tensor([         1,         10,        100,       1000,      10000,     100000,
           1000000,   10000000,  100000000, 1000000000])
2025-03-11 17:36:53 - INFO - ----------------------------------------------------------------------------------------------------
2025-03-11 17:36:53 - INFO - Starting Epoch 1/5
2025-03-11 17:54:44 - INFO - avg Loss: 0.994328793099052
2025-03-11 17:54:44 - INFO - Evaluation start
2025-03-11 17:55:16 - INFO - Mispredictions (up to 5 examples):
2025-03-11 17:55:16 - INFO - Predicted: 350.0, Actual: 315.0
2025-03-11 17:55:16 - INFO - Predicted: 147.0, Actual: 158.0
2025-03-11 17:55:16 - INFO - Predicted: 461.0, Actual: 410.0
2025-03-11 17:55:16 - INFO - Predicted: 370.0, Actual: 405.0
2025-03-11 17:55:16 - INFO - Predicted: 197.0, Actual: 120.0
2025-03-11 17:55:16 - INFO - Epoch                                   1         
2025-03-11 17:55:16 - INFO - Train Loss                              0.9943
2025-03-11 17:55:16 - INFO - Test Loss                               0.7175
2025-03-11 17:55:16 - INFO - Whole Num Acc                           0.935673%
2025-03-11 17:55:16 - INFO - Digit-wise Acc                          58.080808%
2025-03-11 17:55:16 - INFO - MSE                                     1770.595322
2025-03-11 17:55:16 - INFO - R^2                                     0.852900660871659
2025-03-11 17:55:16 - INFO - Learning Rate                           0.000050
2025-03-11 17:55:16 - INFO - ----------------------------------------------------------------------------------------------------
2025-03-11 17:55:16 - INFO - Starting Epoch 2/5
